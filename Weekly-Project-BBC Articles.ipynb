{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s8btHNo7H5Cf"
   },
   "source": [
    "# Organize ML projects with Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8yK9CuWlH5Ch"
   },
   "source": [
    "While Machine Learning is powerful, people often overestimate it: apply machine learning to your project, and all your problems will be solved. In reality, it's not this simple. To be effective, one needs to organize the work very well. In this notebook, we will walkthrough practical aspects of a ML project. To look at the big picture, let's start with a checklist below. It should work reasonably well for most ML projects, but make sure to adapt it to your needs:\n",
    "\n",
    "1. **Define the scope of work and objective**\n",
    "    * How is your solution be used?\n",
    "    * How should performance be measured? Are there any contraints?\n",
    "    * How would the problem be solved manually?\n",
    "    * List the available assumptions, and verify if possible.\n",
    "    \n",
    "    \n",
    "2. **Get the data**\n",
    "    * Document where you can get that data\n",
    "    * Store data in a workspace you can easily access\n",
    "    * Convert the data to a format you can easily manipulate\n",
    "    * Check the overview (size, type, sample, description, statistics)\n",
    "    * Data cleaning\n",
    "    \n",
    "    \n",
    "3. **EDA & Data transformation**\n",
    "    * Study each attribute and its characteristics (missing values, type of distribution, usefulness)\n",
    "    * Visualize the data\n",
    "    * Study the correlations between attributes\n",
    "    * Feature selection, Feature Engineering, Feature scaling\n",
    "    * Write functions for all data transformations\n",
    "    \n",
    "    \n",
    "4. **Train models**\n",
    "    * Automate as much as possible\n",
    "    * Train promising models quickly using standard parameters. Measure and compare their performance\n",
    "    * Analyze the errors the models make\n",
    "    * Shortlist the top three of five most promising models, preferring models that make different types of errors.\n",
    "\n",
    "\n",
    "5. **Fine-tunning**\n",
    "    * Treat data transformation choices as hyperparameters, expecially when you are not sure about them (e.g., replace missing values with zeros or with the median value)\n",
    "    * Unless there are very few hyperparameter value to explore, prefer random search over grid search.\n",
    "    * Try ensemble methods\n",
    "    * Test your final model on the test set to estimate the generalizaiton error. Don't tweak your model again, you would start overfitting the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ofeuKevOH5Ch"
   },
   "source": [
    "## Example: Articles categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m2NSUqUEH5Ci"
   },
   "source": [
    "### Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5GttlMG-H5Cj"
   },
   "source": [
    "Build a model to determine the categories of articles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EwbjWOG1H5Ck"
   },
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1131,
     "status": "ok",
     "timestamp": 1588574452252,
     "user": {
      "displayName": "Minh Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64",
      "userId": "12822549848477954436"
     },
     "user_tz": -420
    },
    "id": "iWq7xex_H5Ck",
    "outputId": "79e56601-3e2b-4f91-f884-3bf3388f162b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m9W7Hzt2H5Cp"
   },
   "outputs": [],
   "source": [
    "bbc = pd.read_csv('https://raw.githubusercontent.com/dhminh1024/practice_datasets/master/bbc-text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 929,
     "status": "ok",
     "timestamp": 1588574456884,
     "user": {
      "displayName": "Minh Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64",
      "userId": "12822549848477954436"
     },
     "user_tz": -420
    },
    "id": "teb1QvD1H5Cs",
    "outputId": "4c34de3c-bec2-4ee4-a1d7-8baa01f37315"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1233</th>\n",
       "      <td>sport</td>\n",
       "      <td>fear will help france - laporte france coach b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>sport</td>\n",
       "      <td>connors  rallying cry for british tennis  do y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>business</td>\n",
       "      <td>us regulator to rule on pain drug us food and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>business</td>\n",
       "      <td>gm pays $2bn to evade fiat buyout general moto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>business</td>\n",
       "      <td>uk bank seals south korean deal uk-based bank ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      category                                               text\n",
       "1233     sport  fear will help france - laporte france coach b...\n",
       "1750     sport  connors  rallying cry for british tennis  do y...\n",
       "59    business  us regulator to rule on pain drug us food and ...\n",
       "759   business  gm pays $2bn to evade fiat buyout general moto...\n",
       "318   business  uk bank seals south korean deal uk-based bank ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1239,
     "status": "ok",
     "timestamp": 1588574461552,
     "user": {
      "displayName": "Minh Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64",
      "userId": "12822549848477954436"
     },
     "user_tz": -420
    },
    "id": "KBW_Sg2RH5Cy",
    "outputId": "b30912a6-ff10-4afc-e33b-d7e65d63a31a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2225 entries, 0 to 2224\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   category  2225 non-null   object\n",
      " 1   text      2225 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 34.9+ KB\n"
     ]
    }
   ],
   "source": [
    "bbc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dh3VRY5Zxmw6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tech', 'business', 'sport', 'entertainment', 'politics'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc.category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = bbc.text\n",
    "category = bbc.category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def preprocessor(text):\n",
    "    \"\"\" Return a cleaned version of text\n",
    "    \"\"\"\n",
    "    # Remove HTML markup\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    # Save emoticons for later appending\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    # Remove any non-word character and append the emoticons,\n",
    "    # removing the nose character for standarization. Convert to lower case\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', ''))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Split a text into list of words\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "# Split a text into list of words and apply stemming technic\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(text, category, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define an object of CountVectorizer() fit and transfom your twits into a 'bag'\n",
    "count = CountVectorizer(stop_words=stop_words,\n",
    "                        tokenizer=tokenizer_porter,\n",
    "                        preprocessor=preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=stop_words,\n",
    "                        tokenizer=tokenizer_porter,\n",
    "                        preprocessor=preprocessor)\n",
    "\n",
    "clf_logistic_tfidf = Pipeline([('vect', tfidf), ('clf', LogisticRegression(random_state=42))])\n",
    "clf_dtree_tfidf = Pipeline([('vect', tfidf), ('clf', DecisionTreeClassifier())])\n",
    "clf_rforest_tfidf = Pipeline([('vect', tfidf), ('clf', RandomForestClassifier())])\n",
    "clf_logistic_count = Pipeline([('vect', count), ('clf', LogisticRegression(random_state=42))])\n",
    "clf_dtree_count = Pipeline([('vect', count), ('clf', DecisionTreeClassifier())])\n",
    "clf_rforest_count = Pipeline([('vect', count), ('clf', RandomForestClassifier())])\n",
    "clf_nb = Pipeline([('vect', count), ('clf', MultinomialNB())])\n",
    "\n",
    "clfs = {\n",
    "    'LogisticRegression tfidf': clf_logistic_tfidf,\n",
    "    'DecisionTree tfidf': clf_dtree_tfidf,\n",
    "    'RandomForest tfidf': clf_rforest_tfidf,\n",
    "    'LogisticRegression count': clf_logistic_count,\n",
    "    'DecisionTree count': clf_dtree_count,\n",
    "    'RandomForest count': clf_rforest_count,\n",
    "    'NaiveBayes': clf_nb\n",
    "}\n",
    "\n",
    "for clf in clfs:\n",
    "    clfs[clf].fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression tfidf\n",
      "accuracy: 0.9760479041916168\n",
      "DecisionTree tfidf\n",
      "accuracy: 0.8308383233532934\n",
      "RandomForest tfidf\n",
      "accuracy: 0.9431137724550899\n",
      "LogisticRegression count\n",
      "accuracy: 0.9640718562874252\n",
      "DecisionTree count\n",
      "accuracy: 0.8188622754491018\n",
      "RandomForest count\n",
      "accuracy: 0.9491017964071856\n",
      "NaiveBayes\n",
      "accuracy: 0.9745508982035929\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Now apply those above metrics to evaluate your model\n",
    "# Your code here\n",
    "for clf in clfs:\n",
    "    predictions = clfs[clf].predict(X_test)\n",
    "    print(clf)\n",
    "    print('accuracy:',accuracy_score(y_test,predictions))\n",
    "#     print('confusion matrix:\\n',confusion_matrix(y_test,predictions))\n",
    "#     print('classification report:\\n',classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting\n",
      "accuracy: 0.9730538922155688\n",
      "confusion matrix:\n",
      " [[158   0   5   0   1]\n",
      " [  4 107   0   0   2]\n",
      " [  2   0 111   0   0]\n",
      " [  0   0   0 146   0]\n",
      " [  3   0   0   1 128]]\n",
      "classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "     business       0.95      0.96      0.95       164\n",
      "entertainment       1.00      0.95      0.97       113\n",
      "     politics       0.96      0.98      0.97       113\n",
      "        sport       0.99      1.00      1.00       146\n",
      "         tech       0.98      0.97      0.97       132\n",
      "\n",
      "     accuracy                           0.97       668\n",
      "    macro avg       0.97      0.97      0.97       668\n",
      " weighted avg       0.97      0.97      0.97       668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "models_comparison = {}\n",
    "\n",
    "base_classifiers = [('LogisticRegression tfidf', clf_logistic_tfidf),\n",
    "                    ('RandomForest tfidf', clf_rforest_tfidf,),\n",
    "                    ('NaiveBayes', clf_nb)]\n",
    "\n",
    "ensembles = {\n",
    "    \"Voting\": VotingClassifier(base_classifiers)\n",
    "}\n",
    "\n",
    "for ensemble in ensembles: \n",
    "    ensembles[ensemble].fit(X_train, y_train)\n",
    "    predictions = ensembles[ensemble].predict(X_test)\n",
    "    print(ensemble)\n",
    "    print('accuracy:',accuracy_score(y_test,predictions))\n",
    "    print('confusion matrix:\\n',confusion_matrix(y_test,predictions))\n",
    "    print('classification report:\\n',classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Weekly-Project-BBC Articles.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
